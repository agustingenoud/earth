{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.add_jars('/app/postgresql-42.1.4.jar')\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"argentinaETL\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *;\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = (\n",
    "StructType().\n",
    "    add(\"ID_Provincia\", IntegerType()).\n",
    "    add(\"Provincia\", StringType()).\n",
    "    add(\"ID_Departamento\", IntegerType()).\n",
    "    add(\"Departamento\", StringType()).\n",
    "    add(\"Id_Cultivo\", IntegerType()).\n",
    "    add(\"Cultivo\", StringType()).\n",
    "    add(\"ID_Campania\", IntegerType()).\n",
    "    add(\"Campania\", StringType()).\n",
    "    add(\"Ha_Sembrada\", DecimalType()).\n",
    "    add(\"Ha_Cosechada\", DecimalType()).\n",
    "    add(\"Produccion_Tn\", DecimalType()).\n",
    "    add(\"Rendimiento_KgxHa\", DecimalType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.\n",
    "    option(\"delimiter\", ';').\n",
    "    csv(\"/dataset/argentina-datos/Estimaciones/Estimaciones.csv\",\n",
    "        schema=schema,\n",
    "        header=True,\n",
    "        ignoreLeadingWhiteSpace=True,\n",
    "        ignoreTrailingWhiteSpace=True,\n",
    "        nullValue='NA')\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"ID_Provincia\", \"id_provincia\") \\\n",
    ".withColumnRenamed(\"Provincia\", \"provincia\") \\\n",
    ".withColumnRenamed(\"ID_Departamento\", \"id_departamento\") \\\n",
    ".withColumnRenamed(\"Departamento\", \"departamento\") \\\n",
    ".withColumnRenamed(\"ID_Cultivo\", \"id_cultivo\") \\\n",
    ".withColumnRenamed(\"Cultivo\", \"cultivo\") \\\n",
    ".withColumnRenamed(\"ID_Campania\", \"id_campania\") \\\n",
    ".withColumnRenamed(\"Campania\", \"campania\") \\\n",
    ".withColumnRenamed(\"Ha_Sembrada\", \"ha_sembrada\") \\\n",
    ".withColumnRenamed(\"Ha_Cosechada\", \"ha_cosechada\") \\\n",
    ".withColumnRenamed(\"Produccion_Tn\", \"produccion_tn\") \\\n",
    ".withColumnRenamed(\"Rendimiento_KgxHa\", \"rendimiento_kgha\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"Ca�a de az�car\",lit(\"Cania de azucar\")).otherwise(df['Cultivo']));\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"T�\", lit(\"Te\")).otherwise(df['Cultivo']));\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"Algod�n\", lit(\"Algodon\")).otherwise(df['Cultivo']));\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"C�rtamo\", lit(\"Cartamo\")).otherwise(df['Cultivo']));\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"Man�\", lit(\"Mani\")).otherwise(df['Cultivo']));\n",
    "df = df.withColumn('cultivo', when(df['cultivo']==\"Ma�z\", lit(\"Maiz\")).otherwise(df['Cultivo']));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_provincia \t Nulls:  0\n",
      "provincia \t Nulls:  0\n",
      "id_departamento \t Nulls:  0\n",
      "departamento \t Nulls:  0\n",
      "id_cultivo \t Nulls:  0\n",
      "cultivo \t Nulls:  0\n",
      "id_campania \t Nulls:  0\n",
      "campania \t Nulls:  0\n",
      "ha_sembrada \t Nulls:  0\n",
      "ha_cosechada \t Nulls:  0\n",
      "produccion_tn \t Nulls:  0\n",
      "rendimiento_kgha \t Nulls:  0\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col, \"\\t\", \"Nulls: \", df.filter(df[col]==\"NA\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('anio', split(df['campania'], '/').getItem(0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('pre', lit(\"-01-01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('fecha',to_date(concat(df['anio'], df['pre']), 'yyyy-mm-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('anio', 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/agricultura\") \\\n",
    "    .option(\"dbtable\", \"agricultura.argentina_datos\") \\\n",
    "    .option(\"user\", \"agricultura\") \\\n",
    "    .option(\"password\", \"p4ssW0rdP4r4Agr1cultur4\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
